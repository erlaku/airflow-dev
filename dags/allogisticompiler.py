# -*- coding: utf-8 -*-
"""allogisticompiler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJH1dGs_ZvGvOPSCGBfuJfO7_yLUqOft

## Compile
"""

import pandas as pd
import numpy as np

url = 'https://docs.google.com/spreadsheets/d/1a38_dvN00Ycq1gv21fqf7Q3q_5ZvxsclC3KR0yD4jJI/export?format=csv&gid='
# df_rbb = pd.read_csv(url+'922405458')
# # df_o2o = pd.read_csv(url+'0')
# df_es = pd.read_csv(url+'1439568429')
# df_ll = pd.read_csv(url+'1874858690')
# df_ex = pd.read_csv(url+'1140683626')
# df_mj = pd.read_csv(url+'1229483984')

# df_o2o = pd.read_csv(url+'1229483984')

# df_24 = pd.read_csv(url+'982555558')

df_24 = pd.read_csv(url+'982555558')

df_24 = df_24[~((df_24['type'].isna()) | (df_24['type'] == '#REF!'))]

df_24['status'] = df_24['status'].fillna('In Progress')
df_24['n_req'] = df_24['n_req'].fillna(1)
df_24['type'] = df_24['type'].str.lower()

df_24['rbb_schedule'] = df_24['rbb_schedule'].apply(lambda x: f"{int(x)}" if pd.notnull(x) else '')

df_24['id'] = df_24['id'].astype('string')

df_24['type_ex'] = df_24['type'].apply(lambda x: x if 'exchange' in x else '')

def transform_type(value):
    value = value.lower().replace(" ", "")
    if 'exchange' in value:
      if 'pickup' in value:
        return 'exchange - pickup'
      else:
        return 'exchange - exchange'
    elif 'maujual' in value:
        return 'mj'
    else:
        return value

df_24['type'] = df_24['type'].apply(transform_type)

def agg_func(group):
    result = group.iloc[0].copy()
    result['n_req'] = group['n_req'].count()
    return result



df_temp = df_24.groupby(['id', 'type', 'pickup0']).last().reset_index()
df_temp['attemptCount'] = 1
df_temp = df_temp.groupby(['id','type'])['attemptCount'].sum().reset_index()

# df_24['attemptCount'] = df_24[df_24['status'] == 'Reschedule'].groupby(['id', 'pickup0'])['status'].transform('count')
# df_24['attemptCount'] = df_24['attemptCount'].fillna(1)

df_24['temp'] = df_24['id'] + df_24['type']
df_temp['temp'] = df_temp['id'] + df_temp['type']

df_temp = df_24.merge(df_temp, on='temp', how='left', suffixes=('', '_temp'))
df_24['attemptCount'] = df_temp['attemptCount'].fillna(1)
df_24.drop(columns='temp', inplace=True)



df_temp = df_24[~df_24['pickup0'].isna()].groupby('id')['pickup0'].min().reset_index()
df_temp = df_24.merge(df_temp, on='id', how='left', suffixes=('', '_temp'))

df_24['pickup0'] = df_temp['pickup0_temp']

df_24 = df_24[~((df_24['status'] == 'Reschedule') & df_24['id'].isin(df_24[df_24['status'] != 'Reschedule']['id']))]


# -------------------------------------------- logistic change -> success --------------------------------------------

r_list = df_24[df_24['status'].isin(['Reschedule', 'In Progress'])]['rbb_schedule'].dropna().unique().tolist()
s_list = df_24[df_24['status'].str.lower() == 'success']['rbb_schedule'].dropna().unique().tolist()

common_list = list(set(r_list).intersection(s_list))

df_24 = df_24[~(df_24['rbb_schedule'].isin(common_list) & (df_24['status'].str.lower() != 'success'))]

# ------------------------------------------------------------------------------------------------------------------------------------

df_24e = df_24.groupby(['id','status','type_ex']).apply(agg_func).reset_index(drop=True)

for i in ['pickup0_fail1', 'pickup0_fail2', 'pickup1_fail0', 'pickup1_fail1', 'pickup1_fail2']:
  df_24e[i] = ''

# df_rbb['type'].fillna('rbb', inplace=True)
# # df_o2o['type'].fillna('o2o', inplace=True)
# df_o2o['type'].fillna('mj', inplace=True)
# df_es['type'].fillna('estore', inplace=True)

# df_ll['type'].fillna('langsunglaku', inplace=True)
# df_ex['type'].fillna('exchange', inplace=True)

# df_mj['type'].fillna('mj', inplace=True)

# df_o2o.status.fillna('onprosess', inplace=True)

# df_ex.loc[df_ex.seller_name == 'Xiaomi Official Store', 'type_ex'] = 'Xiaomi Official Store (Tokopedia)'

# df_log = pd.concat([df_rbb, df_o2o, df_es, df_ll, df_ex, df_mj, df_24e], axis=0).drop_duplicates().reset_index().drop(columns=['index'])
df_log = pd.concat([df_24e], axis=0).drop_duplicates().reset_index().drop(columns=['index'])
df_log.n_req.fillna(1, inplace=True)
df_log.n_rec.fillna(1, inplace=True)
df_log.dropna(subset=['id','pickup0'], inplace=True)
df_log = df_log[df_log.area != '-']
df_log['logistic'] = df_log['logistic'].str.upper().replace(r'.*QIRIMAN.*', 'QIRIMAN', regex=True)

mask = df_log['type_ex'].str.contains(r'estore', case=False, regex=True, na=False)
df_log.loc[mask, 'type'] = 'estore'
df_log.loc[mask, 'type_ex'] = ''
df_log.loc[df_log.type_ex == '', 'type_ex'] = np.nan
df_log.type_ex = df_log.type_ex.str.upper()

df_log.loc[df_log['type'] == 'estore', 'type_ex'] = 'ESTORE'
df_log['type'] = df_log['type'].replace('estore', 'exchange')

status_mapping = {
    'Recieved': 'Success',
    'Partial Unit ': 'Success',
    'No Unit ': 'Fail',
    'Requested': 'In Progress',
    'r2lk6-success': 'Success',
    'Wrong IMEI ?': 'Success',
    'pick up sukses': 'In Progress',
    'Pick up success': 'Success',
    'PIck up Fail': 'Fail',
    'On process': 'In Progress',
    'fraud': 'Fail',
    'Cancel': 'Fail',
    'cancel': 'Fail',
    '-': 'In Progress',
    'reschedule': 'In Progress',
    'Onprosess': 'In Progress',
    'onprosess': 'In Progress',
    'exchange-success': 'Success',
    'Exchange fail buyer': 'Fail',
    'Exchange-success': 'Success',
    'Exchange-fail seller': 'Fail',
    'Exchange-fail': 'Fail',
    'Pickup-success': 'In Progress',
    'Pick up fail': 'Fail',
    'Money in succsess': 'Success',
    'Reschedule': 'In Progress',
    'Pick up waiting': 'In Progress',
    'Exchange-fail courier': 'Fail',
    'Pickup fail': 'Fail',
    'Exchange-otw': 'In Progress',
    'Pending Pick up': 'In Progress',
    'success': 'Success'
}

df_log['status'] = df_log['status'].replace(status_mapping)
df_log.status.fillna('In Progress', inplace=True)

df_log.loc[df_log['status'] != 'Success', 'received0'] = pd.NA

area_mapping = {
    'KOTA JAKARTA UTARA': 'JAKARTA UTARA',
    'KOTA BOGOR ': 'BOGOR',
    ' JAKARTA UTARA': 'JAKARTA UTARA',
    'KOTA TANGERANG SELATAN': 'TANGERANG SELATAN',
    'KOTA MEDAN': 'MEDAN',
    ' KOTA JAKARTA SELATAN': 'JAKARTA SELATAN',
    'KOTA JAKARTA PUSAT': 'JAKARTA PUSAT',
    'KOTA JAKARTA BARAT': 'JAKARTA BARAT',
    'KOTA JAKARTA SELATAN': 'JAKARTA SELATAN',
    'KOTA SURABAYA': 'SURABAYA',
    'BOGOR TIMUR - KOTA': 'BOGOR',
    'KOTA TANGERANG': 'TANGERANG',
    'KOTA DEPOK': 'DEPOK',
    'KOTA BOGOR': 'BOGOR',
    'KOTA BEKASI': 'BEKASI',
    'KOTA BANDUNG': 'BANDUNG',
    'KOTA JAKARTA TIMUR ': 'JAKARTA TIMUR',
    'KOTA JAKARTA TIMUR': 'JAKARTA TIMUR',
    'KOTA JAKARTA SELATAN ': 'JAKARTA SELATAN',
    'KAB. BOGOR': 'BOGOR',
    ' SURABAYA': 'SURABAYA',
    'DKI JAKARTA': 'JAKARTA',
    'KAB. SIDOARDJO': 'SIDOARJO',
    'KOTA MANADO': 'MANADO',
    'KOTA PALEMBANG': 'PALEMBANG',
    'KOTA PONITANAK': 'PONTIANAK',
    'KOTA PONTIANAK': 'PONTIANAK',
    'CIKARANG': 'BEKASI',
    'KOTA JAKARTA PUSAT ': 'JAKARTA PUSAT',
    'KOTA JAKARTA SELATAMN ': 'JAKARTA SELATAN',
    'KOTA JAKARTA UTARA ': 'JAKARTA UTARA',
    'KAB. TANGERANG': 'TANGERANG',
    'KOTA SEMARANG': 'SEMARANG',
    'KAB. SLEMAN': 'SLEMAN',
    'KAB. BADUNG': 'BADUNG',
    'KOTA BANDAR LAMPUNG': 'BANDAR LAMPUNG',
    '-': 'Unknown',
    'KOTA JAYAPURA': 'JAYAPURA',
    'KOTA YOGYAKARTA': 'YOGYAKARTA',
    'KAB. BOYOLALI': 'BOYOLALI',
    'KAB. TULUNGAGUNG': 'TULUNGAGUNG',
    'KAB. PANGKALAN KERINCI': 'PANGKALAN KERINCI',
    'SURAKARTA': 'SOLO',
    'JOGJA': 'YOGYAKARTA'
}

df_log['area'] = df_log['area'].str.upper().replace(area_mapping)

name_mapping = {'ACENG': 'KUSNADI PERMANA',
                'ACENG JABO7': 'KUSNADI PERMANA',
                'ANGGA': 'ANGGA WINANDA',
                'AANG': 'AANG FERDIANSYAH',
                'ANWAR': 'ANWAR EFENDI',
                'BAYU': 'BAYU WIJAKSONO',
                'FARHAN': 'FARHAN ARRIGHI DARMAWAN',
                'DAVID': 'DAVID SANDRA',
                'JALAL': 'JALALUDIN SUYUTI',
                'KUSNADI': 'KUSNADI PERMANA',
                'FAUZI': 'AHMAD FAUZI',
                'YORDAN ALEXANDER': 'YORDAN ALEXANDER BATUALLO',
                'DHAFFA RHESA BUDIAWAN': 'DAFFA RHESA BUDIAWAN',
                'MUSTOFA J': 'JENAL MUSTOFA',
                'RYAN JANUAR': 'RYAN JANUAR ARIANSYAH',
                'HIDAYATULLAH AKSAN': 'AKSAN HIDAYATULLAH',
                '0': np.nan,
                'FATHUR CS': 'FATHUR',
                'GILANG PRADANA': 'GILANG PRADANA SAPUTRA',
                'HQ PROSES': np.nan,
                'ISNAN ISTIGFARIAN': 'ISNAN ISTIGHFARIAN',
                'MUHAMMAD IRPAN ARDIAN ': 'MUHAMMAD IRPAN ARDIAN',
                'M IRPAN ARDIAN': 'MUHAMMAD IRPAN ARDIAN',
                'RAMLAN KUNAEDI': 'RAMLAN KURNAEDI',
                'REQUEST CS MAUJUAL': np.nan,
                'TELESALES': np.nan,
                'FAIZ AL HADI': 'A. FAIZ AL HADI',
                'MUHAMAD YOGI FAHLEPI': 'YOGI FAHLEPI'

                }

df_log['courier_name'] = df_log['courier_name'].str.upper().replace(name_mapping)
df_log['courier_name'] = df_log['courier_name'].str.upper().replace('.*QIRIMAN.*', np.nan, regex=True)

mask = df_log.groupby(['pickup0','courier_name']).size().reset_index()['pickup0'] + ' ' + df_log.groupby(['pickup0','courier_name']).size().reset_index()['courier_name']
mask = mask.tolist()

def val0(x):
  global mask
  out = str(x['pickup0'])+' '+str(x['courier_name'])
  if out in mask:
    mask.remove(out)
    return x['courier_name']

df_log['val0'] = df_log.apply(val0, axis=1)

mask_1 = df_log.groupby(['pickup0','courier_name','type']).size().reset_index()['pickup0'] + ' ' + df_log.groupby(['pickup0','courier_name','type']).size().reset_index()['courier_name'] + ' ' + df_log.groupby(['pickup0','courier_name','type']).size().reset_index()['type']
mask_1 = mask_1.tolist()

def val2(x):
  global mask_1
  out = str(x['pickup0'])+' '+str(x['courier_name'])+' '+str(x['type'])
  if out in mask_1:
    mask_1.remove(out)
    return x['courier_name']

df_log['val2'] = df_log.apply(val2, axis=1)

area_mapping = {
    'SEMARANG': 'JAWA TENGAH',
    'JAKARTA BARAT': 'JABODETABEK',
    'BOGOR': 'JABODETABEK',
    'BEKASI': 'JABODETABEK',
    'SURABAYA': 'JAWA TIMUR',
    'BANDUNG': 'JAWA BARAT',
    'TANGERANG': 'JABODETABEK',
    'JAKARTA PUSAT': 'JABODETABEK',
    'JAKARTA UTARA': 'JABODETABEK',
    'JAKARTA SELATAN': 'JABODETABEK',
    'MEDAN': 'SUMATRA UTARA',
    'SLEMAN': 'DI YOGYAKARTA',
    'DEPOK': 'JABODETABEK',
    'TANGERANG SELATAN': 'JABODETABEK',
    'JAKARTA TIMUR': 'JABODETABEK',
    'CIREBON': 'JAWA BARAT',
    'MALANG': 'JAWA TIMUR',
    'MOJOKERTO': 'JAWA TIMUR',
    'PURWAKARTA': 'JAWA BARAT',
    'SIDOARJO': 'JAWA TIMUR',
    'TEGAL': 'JAWA TENGAH',
    'KARAWANG': 'JAWA BARAT',
    'INDRAGIRI HULU': 'RIAU',
    'LHOKSEUMAWE': 'ACEH',
    'BANDA ACEH': 'ACEH',
    'KUNINGAN': 'JAWA BARAT',
    'GORONTALO': 'GORONTALO',
    'KUDUS': 'JAWA TENGAH',
    'INDRAMAYU': 'JAWA BARAT',
    'GIANYAR': 'BALI',
    'KEDIRI': 'JAWA TIMUR',
    'PALEMBANG': 'SUMATRA SELATAN',
    'SUKOHARJO': 'JAWA TENGAH',
    'DENPASAR': 'BALI',
    'MAKASSAR': 'SULAWESI SELATAN',
    'MANADO': 'SULAWESI UTARA',
    'SUBANG': 'JAWA BARAT',
    'PONTIANAK': 'KALIMANTAN BARAT',
    'PEKANBARU': 'RIAU',
    'BANDAR LAMPUNG': 'LAMPUNG',
    'SOLO': 'JAWA TENGAH',
    'GARUT': 'JAWA BARAT',
    'PADANG': 'SUMATRA BARAT',
    'BALIKPAPAN': 'KALIMANTAN TIMUR',
    'CILEGON': 'BANTEN',
    'JAMBI': 'JAMBI',
    'MAGELANG': 'JAWA TENGAH',
    'SERANG': 'BANTEN',
    'TASIKMALAYA': 'JAWA BARAT',
    'BANYUMAS': 'JAWA TENGAH',
    'PALU': 'SULAWESI TENGAH',
    'BANJARMASIN': 'KALIMANTAN SELATAN',
    'PANGKAL PINANG': 'BANGKA BELITUNG',
    'YOGYAKARTA': 'DI YOGYAKARTA',
    'SUKABUMI': 'JAWA BARAT',
    'MIMIKA': 'PAPUA',
    'SUMBA TIMUR': 'NUSA TENGGARA TIMUR',
    'BADUNG': 'BALI',
    'JAYAPURA': 'PAPUA',
    'ACEH TENGAH': 'ACEH',
    'DELI SERDANG': 'SUMATRA UTARA',
    'LEBAK': 'BANTEN',
    'GRESIK': 'JAWA TIMUR',
    'DUMAI': 'RIAU',
    'JEMBER': 'JAWA TIMUR',
    'SALATIGA': 'JAWA TENGAH',
    'SORONG': 'PAPUA BARAT',
    'ENDE': 'NUSA TENGGARA TIMUR',
    'BANDUNG BARAT': 'JAWA BARAT',
    'AMBON': 'MALUKU',
    'SUMEDANG': 'JAWA BARAT',
    'BATU': 'JAWA TIMUR',
    'LUBUK LINGGAU': 'SUMATRA SELATAN',
    'PALOPO': 'SULAWESI SELATAN',
    'TABANAN': 'BALI',
    'BANJARBARU': 'KALIMANTAN SELATAN',
    'KOTABARU': 'KALIMANTAN SELATAN',
    'LABUHANBATU': 'SUMATRA UTARA',
    'BANYUWANGI': 'JAWA TIMUR',
    'KOLAKA': 'SULAWESI TENGGARA',
    'PEKALONGAN': 'JAWA TENGAH',
    'KUPANG': 'NUSA TENGGARA TIMUR',
    'JEMBRANA': 'BALI',
    'BLITAR': 'JAWA TIMUR',
    'BENGKULU': 'BENGKULU',
    'BULELENG': 'BALI',
    'OGAN KOMERING ULU': 'SUMATRA SELATAN',
    'GUNUNGSITOLI': 'NUSA TENGGARA BARAT',
    'BENGKALIS': 'RIAU',
    'KENDARI': 'SULAWESI TENGGARA',
    'TERNATE': 'MALUKU UTARA',
    'MALUKU TENGGARA BARAT': 'MALUKU',
    'ASAHAN': 'SUMATRA UTARA',
    'PEMATANG SIANTAR': 'SUMATRA UTARA',
    'CIMAHI': 'JAWA BARAT',
    'KOTAMOBAGU': 'SULAWESI UTARA',
    'BANGKA TENGAH': 'BANGKA BELITUNG',
    'MADIUN': 'JAWA TIMUR',
    'TULUNGAGUNG': 'JAWA TIMUR',
    'SAMARINDA': 'KALIMANTAN TIMUR',
    'SIKKA': 'NUSA TENGGARA TIMUR',
    'NUSA TENGGARA TIMUR': 'NUSA TENGGARA TIMUR',
    'METRO': 'LAMPUNG',
    'GOWA': 'SULAWESI SELATAN',
    'TANAH BUMBU': 'KALIMANTAN SELATAN',
    'KLUNGKUNG': 'BALI',
    'MATARAM': 'NUSA TENGGARA BARAT',
    'TEBING TINGGI': 'SUMATRA UTARA',
    'NABIRE': 'PAPUA',
    'BUKITTINGGI': 'SUMATRA BARAT',
    'SIMALUNGUN': 'SUMATRA UTARA',
    'PULANG PISAU': 'KALIMANTAN TENGAH',
    'KEBUMEN': 'JAWA TENGAH',
    'TANAH LAUT': 'KALIMANTAN SELATAN',
    'JAKARTA': 'JABODETABEK',
    'BATAM': 'KEPULAUAN RIAU',
    'RIAU': 'RIAU',
    'TAPANULI': 'SUMATRA UTARA',
    'BENAWA': 'SUMATRA SELATAN',
    'BINJAI': 'SUMATRA UTARA',
    'PAMEKASAN': 'JAWA TIMUR',
    'JEPARA': 'JAWA TENGAH',
    'SRAGEN': 'JAWA TENGAH',
    'MINAHASA': 'SULAWESI UTARA',
    'KOTAWARINGIN': 'KALIMANTAN TENGAH',
    'PROBOLINGGO': 'JAWA TIMUR',
    'CILACAP': 'JAWA TENGAH',
    'BALI': 'BALI',
    'HALMAHERA': 'MALUKU UTARA',
    'BOYOLALI': 'JAWA TENGAH',
    'TULUNG AGUNG': 'JAWA TIMUR',
    'PANGKALAN KERINCI': 'RIAU',
    'TANGERANG SELATAN': 'JABODETABEK',
    'MANGGARAI': 'NUSA TENGGARA TIMUR',
    'BANJAR': 'JAWA TENGAH & DI YOGYAKARTA',
    'SAMOSIR': 'SUMATRA',
    'BERAU': 'KALIMANTAN',
    'MEDAN PETISAH': 'SUMATRA',
    'PENJARINGAN': 'JABODETABEK',
    'TANGERANG SELATAN': 'JABODETABEK'
}

df_log['val1'] = df_log.area.str.upper().map(area_mapping)

area_mapping = {
    'SUMATRA UTARA': 'SUMATRA',
    'DI YOGYAKARTA': 'JAWA TENGAH & DI YOGYAKARTA',
    'RIAU': 'SUMATRA',
    'ACEH': 'SUMATRA',
    'GORONTALO': 'SULAWESI',
    'BALI': 'BALI & NUSA TENGGARA',
    'SUMATRA SELATAN': 'SUMATRA',
    'SULAWESI SELATAN': 'SULAWESI',
    'SULAWESI UTARA': 'SULAWESI',
    'KALIMANTAN BARAT': 'KALIMANTAN',
    'LAMPUNG': 'SUMATRA',
    'SUMATRA BARAT': 'SUMATRA',
    'KALIMANTAN TIMUR': 'KALIMANTAN',
    'BANTEN': 'JAWA BARAT & BANTEN',
    'JAMBI': 'SUMATRA',
    'SULAWESI TENGAH': 'SULAWESI',
    'KALIMANTAN SELATAN': 'KALIMANTAN',
    'BANGKA BELITUNG': 'SUMATRA',
    'PAPUA': 'PAPUA & MALUKU',
    'NUSA TENGGARA TIMUR': 'BALI & NUSA TENGGARA',
    'PAPUA BARAT': 'PAPUA & MALUKU',
    'MALUKU': 'PAPUA & MALUKU',
    'SULAWESI TENGGARA': 'SULAWESI',
    'BENGKULU': 'SUMATRA',
    'NUSA TENGGARA BARAT': 'BALI & NUSA TENGGARA',
    'MALUKU UTARA': 'PAPUA & MALUKU',
    'KALIMANTAN TENGAH': 'KALIMANTAN',
    'KEPULAUAN RIAU': 'SUMATRA',
    'JAWA TENGAH': 'JAWA TENGAH & DI YOGYAKARTA',
    'JAWA BARAT': 'JAWA BARAT & BANTEN'
}

df_log['val1'] = df_log.val1.replace(area_mapping)

salary_mapping = {
    'AGUSTINUS OSVALDO RIOS MALAU': 125000,
    'AHMAD FAUZI': 128846,
    'AKSAN HIDAYATULLAH': 100000,
    'ALDI ARDIANA': 125000,
    'ALFIN RAHMAD ZAKARIA': 125000,
    'AMIRRUDIN': 128846,
    'AZI SETIAWAN': 100000,
    'BASUNDARA DEVITO': 100000,
    'BAYU WIJAKSONO': 128846,
    'DAFFA RHESA BUDIAWAN': 128846,
    'DARKONI': 128846,
    'DAVID SANDRA': 128846,
    'FAIZ AL HADI': 125000,
    'FATHUDDIN YUSUF': 128846,
    'FIRDAUS': 128846,
    'FURKON RAMADANI': 100000,
    'GILANG PRADANA SAPUTRA': 128846,
    'HADI MURDIANSYAH': 128846,
    'HIJANI TRIADI ACHMAD': 125000,
    'IRFAN MAULANA YUSUP': 100000,
    'ISNAN ISTIGHFARIAN': 128846,
    'JENAL MUSTOFA': 128846,
    'KRISHARYANTO': 100000,
    'MARULITUA MANALU': 100000,
    'MOHAMMAD IQBAL': 128846,
    'MUHAMAD RIDWAN': 100000,
    'MUHAMMAD IRPAN ARDIAN': 100000,
    'PUAD MAHPUDIN': 100000,
    'PURNAMA SUDARYANTO': 0,
    'RAMLAN KURNAEDI': 100000,
    'RANDI PRIANDI': 128846,
    'RYAN JANUAR ARIANSYAH': 100000,
    'YOGI FAHLEPI': 100000,
    'YORDAN ALEXANDER BATUALLO': 128846,
    'A. FAIZ AL HADI': 125000,
    'AANG FERDIANSYAH': 125000,
    'ANGGA WINANDA': 125000,
    'ANWAR EFENDI': 125000,
    'BIMA': 125000,
    'DIMAS': 125000,
    'ELZA': 125000,
    'FARHAN ARRIGHI DARMAWAN': 125000,
    'FATHUR': 125000,
    'FIKRI': 125000,
    'FIRMAN': 125000,
    'HANIFAL QALBY': 100000,
    'HENDI KRISDIANSYAH': 125000,
    'JALALUDIN SUYUTI': 125000,
    'KUSNADI PERMANA': 125000,
    'MUHAMAD SALIM': 125000,
    'MUHAMMAD HADYAN': 100000,
    'PEBRO RANGGA ARDI': 125000,
    'PURNAMA SUDARYANTO': 100000
}

allowance_mapping = {
    'AGUSTINUS OSVALDO RIOS MALAU': 34500,
    'AHMAD FAUZI': 50500,
    'AKSAN HIDAYATULLAH': 20000,
    'ALDI ARDIANA': 34500,
    'ALFIN RAHMAD ZAKARIA': 34500,
    'AMIRRUDIN': 50500,
    'AZI SETIAWAN': 20000,
    'BASUNDARA DEVITO': 20000,
    'BAYU WIJAKSONO': 50500,
    'DAFFA RHESA BUDIAWAN': 50500,
    'DARKONI': 50500,
    'DAVID SANDRA': 50500,
    'FAIZ AL HADI': 34500,
    'FATHUDDIN YUSUF': 50500,
    'FIRDAUS': 50500,
    'FURKON RAMADANI': 20000,
    'GILANG PRADANA SAPUTRA': 50500,
    'HADI MURDIANSYAH': 50500,
    'HIJANI TRIADI ACHMAD': 34500,
    'IRFAN MAULANA YUSUP': 20000,
    'ISNAN ISTIGHFARIAN': 50500,
    'JENAL MUSTOFA': 50500,
    'KRISHARYANTO': 20000,
    'MARULITUA MANALU': 20000,
    'MOHAMMAD IQBAL': 50500,
    'MUHAMAD RIDWAN': 20000,
    'MUHAMMAD IRPAN ARDIAN': 20000,
    'PUAD MAHPUDIN': 20000,
    'PURNAMA SUDARYANTO': 50500,
    'RAMLAN KURNAEDI': 20000,
    'RANDI PRIANDI': 50500,
    'RYAN JANUAR ARIANSYAH': 20000,
    'YOGI FAHLEPI': 20000,
    'A. FAIZ AL HADI': 34500,
    'AANG FERDIANSYAH': 34500,
    'ANGGA WINANDA': 34500,
    'ANWAR EFENDI': 34500,
    'BIMA': 34500,
    'DIMAS': 34500,
    'ELZA': 34500,
    'FARHAN ARRIGHI DARMAWAN': 34500,
    'FATHUR': 34500,
    'FIKRI': 34500,
    'FIRMAN': 34500,
    'HANIFAL QALBY': 20000,
    'HENDI KRISDIANSYAH': 34500,
    'JALALUDIN SUYUTI': 34500,
    'KUSNADI PERMANA': 34500,
    'MUHAMAD SALIM': 34500,
    'MUHAMMAD HADYAN': 20000,
    'PEBRO RANGGA ARDI': 34500,
    'PURNAMA SUDARYANTO': 20000,
    'YORDAN ALEXANDER BATUALLO': 50500
}

monthly_mapping = {
    'AGUSTINUS OSVALDO RIOS MALAU': 'Daily Worker',
    'AHMAD FAUZI': 'Contract',
    'AKSAN HIDAYATULLAH': 'Daily Worker',
    'ALDI ARDIANA': 'Daily Worker',
    'ALFIN RAHMAD ZAKARIA': 'Daily Worker',
    'AMIRRUDIN': 'Contract',
    'AZI SETIAWAN': 'Daily Worker',
    'BASUNDARA DEVITO': 'Daily Worker',
    'BAYU WIJAKSONO': 'Contract',
    'DAFFA RHESA BUDIAWAN': 'Contract',
    'DARKONI': 'Contract',
    'DAVID SANDRA': 'Contract',
    'FAIZ AL HADI': 'Daily Worker',
    'FATHUDDIN YUSUF': 'Contract',
    'FIRDAUS': 'Contract',
    'FURKON RAMADANI': 'Daily Worker',
    'GILANG PRADANA SAPUTRA': 'Contract',
    'HADI MURDIANSYAH': 'Contract',
    'HIJANI TRIADI ACHMAD': 'Daily Worker',
    'IRFAN MAULANA YUSUP': 'Daily Worker',
    'ISNAN ISTIGHFARIAN': 'Contract',
    'JENAL MUSTOFA': 'Contract',
    'KRISHARYANTO': 'Daily Worker',
    'MARULITUA MANALU': 'Daily Worker',
    'MOHAMMAD IQBAL': 'Contract',
    'MUHAMAD RIDWAN': 'Daily Worker',
    'MUHAMMAD IRPAN ARDIAN': 'Daily Worker',
    'PUAD MAHPUDIN': 'Daily Worker',
    'PURNAMA SUDARYANTO': 'Daily Worker',
    'RAMLAN KURNAEDI': 'Daily Worker',
    'RANDI PRIANDI': 'Contract',
    'RYAN JANUAR ARIANSYAH': 'Daily Worker',
    'YOGI FAHLEPI': 'Daily Worker',
    'YORDAN ALEXANDER BATUALLO': 'Contract',
    'A. FAIZ AL HADI': 'Daily Worker',
    'AANG FERDIANSYAH': 'Daily Worker',
    'ANGGA WINANDA': 'Daily Worker',
    'ANWAR EFENDI': 'Daily Worker',
    'BIMA': 'Daily Worker',
    'DIMAS': 'Daily Worker',
    'ELZA': 'Daily Worker',
    'FARHAN ARRIGHI DARMAWAN': 'Daily Worker',
    'FATHUR': 'Daily Worker',
    'FIKRI': 'Daily Worker',
    'FIRMAN': 'Daily Worker',
    'HANIFAL QALBY': 'Daily Worker',
    'HENDI KRISDIANSYAH': 'Daily Worker',
    'JALALUDIN SUYUTI': 'Daily Worker',
    'KUSNADI PERMANA': 'Daily Worker',
    'MUHAMAD SALIM': 'Daily Worker',
    'MUHAMMAD HADYAN': 'Daily Worker',
    'PEBRO RANGGA ARDI': 'Daily Worker',
    'PURNAMA SUDARYANTO': 'Daily Worker'
}

df_log['courier_salary'] = df_log['val2'].map(salary_mapping).fillna('')
df_log['courier_allowance'] = df_log['val2'].map(allowance_mapping).fillna('')
df_log['courier_monthly'] = df_log['courier_name'].map(monthly_mapping).fillna('')
# df_log['courier_salary_val0'] = df_log['val0'].map(salary_mapping).fillna('')
# df_log['courier_allowance_val0'] = df_log['val0'].map(allowance_mapping).fillna('')

# set date range

df_log = df_log[(pd.to_datetime(df_log.pickup0) >= pd.Timestamp('2024-01-01')) &
                (pd.to_datetime(df_log['pickup0']) <= pd.Timestamp.now().normalize())]

import datetime

ct = datetime.datetime.now()+ datetime.timedelta(hours=7)
# df_log.loc[0, 'last_update'] = str(ct.strftime('%b, %d %Y %H:%M:%S'))
df_log['last_update'] = str(ct.strftime('%b, %d %Y %H:%M:%S'))

#swap
# mask = (df_log['pickup0'] > df_log['received0'])

# df_log.loc[mask, ['pickup0', 'received0']] = df_log.loc[mask, ['received0', 'pickup0']].values
# df_log['pickup0'], df_log['received0'] = np.where(mask, [df_log['received0'], df_log['pickup0']], [df_log['pickup0'], df_log['received0']])

df_log['pickup1'] = ''
df_log['received1'] = ''

df_log['pickup0'] = pd.to_datetime(df_log['pickup0'])
df_log['pickup1'] = pd.to_datetime(df_log['pickup1'])
df_log['received0'] = pd.to_datetime(df_log['received0'])
df_log['received1'] = pd.to_datetime(df_log['received1'])

df_log['pickup0'], df_log['received0'] = np.where(df_log['pickup0'] > df_log['received0'], [df_log['received0'], df_log['pickup0']], [df_log['pickup0'], df_log['received0']])

df_log.pickup0_fail0 = df_log.pickup0_fail0.replace('0','')

df_log['week'] = df_log['pickup0'].dt.isocalendar().week

df_log.loc[df_log['status'] == 'In Progress', 'received0'] = ct

df_log['received1'] = df_log['received0']
df_log['pickup1'] = df_log['pickup0']

"""## Export to spreadsheet"""

df_3pl_cost = pd.read_csv('https://docs.google.com/spreadsheets/d/16Nk5NHKQYj4DGhivT9obUqjNdPd3VcxFXD0GFRTNrcY/export?format=csv&gid=1816466187')
df_3pl_cost = df_3pl_cost[df_3pl_cost['type'] == 'inbound']
df_3pl_cost = df_3pl_cost[df_3pl_cost['cost'].notna()]
df_3pl_cost['cost'] = df_3pl_cost['cost'].str.replace(',','').astype(int)

log_area_list = df_log[df_log['area'].notna()]['area'].tolist()
log_area_list.append('Surakartaa')

def define_area(x):
  for i in log_area_list:
    if x.upper() == 'LAMPUNG':
      return 'LAMPUNG'
    elif (x.lower()+'_' in i.lower()+'_'):
      return i
    elif (i.lower()+'_' in x.lower()+'_'):
      return i

  return None

df_3pl_cost['log_area'] = df_3pl_cost['area'].apply(define_area)

df_log['area_lower'] = df_log['area'].str.strip().str.lower()
df_log['logistic_lower'] = df_log['logistic'].str.strip().str.lower()
df_3pl_cost['area_lower'] = df_3pl_cost['log_area'].str.strip().str.lower()
df_3pl_cost['logistic_lower'] = df_3pl_cost['logistic'].str.strip().str.lower()

cost_mapping = df_3pl_cost.set_index(['area_lower', 'logistic_lower'])['cost'].to_dict()

cost_mapping = {k: v for k, v in cost_mapping.items() if not pd.isna(k[0])}

def get_3pl_cost(row):
    return cost_mapping.get((row['area_lower'], row['logistic_lower']), np.nan)

df_log['3pl_cost'] = df_log.apply(get_3pl_cost, axis=1)


# df_log.loc[df_log['logistic'].str.contains('indopaket', case=False, na=False), '3pl_cost'] = 44500

df_log.drop(columns=['area_lower', 'logistic_lower'], inplace=True)

# from google.colab import auth
# auth.authenticate_user()

# import gspread
# from google.auth import default
# creds, _ = default()

# gc = gspread.authorize(creds)

# from gspread_dataframe import set_with_dataframe

# sh = gc.open('allogistic')
# set_with_dataframe(sh.get_worksheet(0), df_log)

# df_log.set_index('id').to_csv('/content/drive/MyDrive/allogistic.csv')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install authlib
# !pip install google-api-python-client oauth2client

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %env SHEET_TYPE = service_account
# %env SHEET_PROJECT_ID = macro-polymer-399408
# %env SHEET_PRIVATE_KEY_ID = e28490a79be94bc7ac7e1062583e21178cc90eaf
# %env SHEET_PRIVATE_KEY = -----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQC+zbq5R+4YTmgL\nWm6/gwYHFp/L3mvppBRjTOKdrGK+SrDf2rL+85DbOVLRyGAZ5GaKgTUaGf/KiBt+\nn7MFFN1GcNPj0hNc3UYKcyi3l66JUdHpdLi8Hp53M7qa+K2Uc2rh+O96Fkc91ZN+\nRKkvNfTiBysdTXCITPJrhoZ0P7b5fwqXKuAPxtqtq1jJPE8utkf25fPjvjPGf9NY\nzBY/zGq2edvTknLd8RiWP8y929yAwRr1XJIgmRi4g/xS9hfQdgdom30JsQ96lB0x\neU4ebe6K6uspOgAaN4nGkmRvLjWXUp5tuea0dBwtgfKTC1p1Y+ry9EpWxMY9dz6A\n509kVSIHAgMBAAECggEATt6XepMDFkUCuP6unLc6PJ/bedf8310VmF5lpitlvab+\nKeoq5zwi+ptdYp3jK5C/2izCmIF6vGs6gkLdKxKOzNlCnP9vRYmptBQaFpm6acTK\ncifvXMODWV6mmfyFM8HpwnK1+s9Or36jMySKkptJGE1CQyCJja3Q3yWCWncj2Uk1\nhvsx5WcbjDyDiTV5o1d9KuuXWz3escTbvt9gfsL2Rek7PvCd6UjIHsSLB9WT1RQB\nl7Jr2eIHOTsN0AuOVK0RNT39mkqXT/bbfX1yNxmU/5xn7r/Civpw1r7Xnl223HLU\n7kSsKZKwv+yoT8jbH+3VyIU8LUGeHDS5uyl/IJDdQQKBgQDhC8cdG4go4P0A03OJ\nhcjJty/BOlMlwAHyI7xvBNvbzKk2+XB+/SQ1BML1QigdvrSrjUtBrnI9TeO0Rv9S\noe1/xbyNzHdZkz8Hk0ClUteJJKGu+rK1zTWdOSPn1ymmbC7bsGoUKwMmpr5YPEx1\nuoIBerlwVPkyeeiCkgOCZo6c9wKBgQDZDDlSpRUWM8x0ESRQxlgkDewKkS1FezQF\n84sTczqZOXbSBXi23rb9UHcsdDFu3JXtEz6VfA53N0Wz9IPa25sg5KGiS3SMxaap\nbm7xXa+lQFJZY+LPgDK+jsHyC7EfizYK9vq2OPAwyngm6aAKRt2Mf1Ofc0XFfco2\ngB/cslOvcQKBgCbwrsdumCTaqWyXKgrFx3R/6hd5TCHgE3YekOB06sYhp7YHe1u9\n2aIC7OkbRW3ALerxS/BVixbof1oWXt+hfUL6qCGxG4W4YNOiuzDgIMNuPzvTzVB/\n7aMC3B+lr2NInOMOp4xN1QG/IHiMFn7Ygt5Wqfex/Fyrk3vkYA8UR+EJAoGBAISh\n46WNG4WlwZprnzV8/v+JQmRysqdy0ieQ27vW/kgDxYgxr8KP9mJ1eV3A4zRLt5JX\n4DaYyH4xWHbPRIPJDtLa46q5UU90uJNw2HrGNT9WPvxA/aivPlMUCZBPdL1B5JRu\nuGYCBUTLdQDSYunkxB3G8OLPHiPA6oDDarUChAEBAoGBAIBBEqcRk5LWUkl19cat\nD8+AZfR5Zs6sQzOCh7I6JcY9mGHlNa6Cu+hAVJRo5mIZkQClNcZE7Jk9lCuf2lGd\nngWHVlgAzRz0PsrtDZsb8o6k4Oa3GG65DdOwsixjji5/i10UNMgoFn22bvwmnlvk\nN5goLfz696qMuYmqOVB1Mpka\n-----END PRIVATE KEY-----\n
# %env SHEET_CLIENT_EMAIL = ops-864@macro-polymer-399408.iam.gserviceaccount.com
# %env SHEET_CLIENT_ID = 113749509940777766546
# %env SHEET_AUTH_URI = https://accounts.google.com/o/oauth2/auth
# %env SHEET_TOKEN_URI = https://oauth2.googleapis.com/token
# %env SHEET_AUTH_PROVIDER_X509_CERT_URL = https://www.googleapis.com/oauth2/v1/certs
# %env SHEET_CLIENT_X509_CERT_URL = https://www.googleapis.com/robot/v1/metadata/x509/ops-864%40macro-polymer-399408.iam.gserviceaccount.com

import os

def create_keyfile_dict():
    variables_keys = {
        "type": os.environ.get("SHEET_TYPE"),
        "project_id": os.environ.get("SHEET_PROJECT_ID"),
        "private_key_id": os.environ.get("SHEET_PRIVATE_KEY_ID"),
        "private_key": os.environ.get("SHEET_PRIVATE_KEY").replace("\\n", "\n"),
        "client_email": os.environ.get("SHEET_CLIENT_EMAIL"),
        "client_id": os.environ.get("SHEET_CLIENT_ID"),
        "auth_uri": os.environ.get("SHEET_AUTH_URI"),
        "token_uri": os.environ.get("SHEET_TOKEN_URI"),
        "auth_provider_x509_cert_url": os.environ.get("SHEET_AUTH_PROVIDER_X509_CERT_URL"),
        "client_x509_cert_url": os.environ.get("SHEET_CLIENT_X509_CERT_URL"),
    }
    return variables_keys

import gspread
from gspread_dataframe import set_with_dataframe
from gspread import Client
from oauth2client.service_account import ServiceAccountCredentials

scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']

creds = ServiceAccountCredentials.from_json_keyfile_dict(create_keyfile_dict(), scope)

client = gspread.authorize(creds)

# df_log.reset_index(drop=True, inplace=True)

# df_log['index'] = df_log.index
# columns = list(df_log.columns)
# columns.append(columns.pop(columns.index('index')))
# df_log = df_log[columns]

sheet = client.open("allogistic")
sheet.worksheet('data').clear()
set_with_dataframe(sheet.get_worksheet(0), df_log)

"""#### additional"""

df_is = pd.read_csv('https://docs.google.com/spreadsheets/d/16Nk5NHKQYj4DGhivT9obUqjNdPd3VcxFXD0GFRTNrcY/export?format=csv&gid=1601826919')
df_is['imei_list'] = df_is['imei_list'].astype(str)

df_is['postal_code'] = df_is['postal_code'].apply(lambda x: f"{int(x)}" if pd.notnull(x) else '')

df_1 = df_is[df_is['delivery_service_name'].str.contains('lakusend|-|QIRIMAN', case=False, na=False) &
        (df_is['delivery_service_name'] != 'REX - TV')]
df_2 = df_is[~df_is['delivery_service_name'].str.contains('lakusend|-|QIRIMAN', case=False, na=False) |
             df_is['delivery_service_name'].str.contains('REX - TV', case=False, na=False)]

df_a = df_log.copy()

df_a.drop_duplicates(subset=['id', 'rbb_schedule'], keep='last', inplace=True)
df_a = df_a[['id', 'rbb_schedule', 'logistic']]

df_merge = df_1.merge(df_a[['id', 'logistic']], left_on='tracking_number', right_on='id', how='left', suffixes=('', '_track'))



imei_to_logistic = dict(zip(df_a['rbb_schedule'], df_a['logistic']))

def get_logistic(imei_list, imei_to_logistic):
    for imei in imei_list.split(','):
        imei = imei.strip()
        if imei in imei_to_logistic:
            return imei_to_logistic[imei]
    return None

df_merge['logistic_x'] = df_merge['imei_list'].apply(get_logistic, args=(imei_to_logistic,))
df_merge['logistic'] = df_merge['logistic'].fillna(df_merge['logistic_x'])

df_merge.rename(columns={'logistic': 'fflog_logistic'}, inplace=True)


# df_merge['delivery_service_name'] = df_merge['fflog_logistic'].fillna(df_merge['delivery_service_name'])

def check_condition(row):
    delivery_service_name = row['delivery_service_name']
    fflog_logistic = row['fflog_logistic']

    # Handle None/NaN values
    if pd.isna(delivery_service_name):
        delivery_service_name = ''
    if pd.isna(fflog_logistic):
        fflog_logistic = ''

    # Check if both contain 'qiriman'
    if 'qiriman' in delivery_service_name.lower() and 'qiriman' in fflog_logistic.lower():
        return False  # Do not update
    return True  # Update

condition = df_merge.apply(check_condition, axis=1)
df_merge['delivery_service_name'] = np.where(condition, df_merge['fflog_logistic'].fillna(df_merge['delivery_service_name']), df_merge['delivery_service_name'])

df_merge.drop(columns=['logistic_x', 'fflog_logistic'], inplace=True)

# df_merge[df_merge['delivery_service_name'].str.contains('QIRIMAN')]

final_result = pd.concat([df_merge, df_2])

final_result['area_lower'] = final_result['area'].str.strip().str.lower()
final_result['logistic_lower'] = final_result['delivery_service_name'].str.strip().str.lower()

final_result['3pl_cost'] = final_result.apply(get_3pl_cost, axis=1)

final_result.loc[final_result['delivery_service_name'].str.contains('indopaket', case=False, na=False), '3pl_cost'] = 44500

final_result = final_result.reset_index(drop=True)

final_result.loc[final_result['delivery_service_name'] == 'QIRIMAN', 'delivery_service_name'] = final_result['delivery_service_name'] + ' ' + final_result['area']

final_result.drop(columns=['area_lower', 'logistic_lower'], inplace=True)



df_indopaket_cost = pd.read_csv('https://docs.google.com/spreadsheets/d/16Nk5NHKQYj4DGhivT9obUqjNdPd3VcxFXD0GFRTNrcY/export?format=csv&gid=1497570630')
df_indopaket_cost['postal_code'] = df_indopaket_cost['postal_code'].astype(str)

final_result = final_result.merge(df_indopaket_cost, on='postal_code', how='left')


final_result.loc[final_result['delivery_service_name'].str.contains('indopaket', case=False, na=False), '3pl_cost'] = final_result['indopaket_cost']

# final_result[final_result['imei_list'] == '864763068869062']
# df_merge[df_merge['tracking_number'] == 'INTKP20240524820916_B']

# for i, j in cost_mapping_cleaned:
#   if cost_mapping_cleaned[i,j] == 270500:
#     print(i, j)

# # len(cost_mapping)
# # cost_mapping_cleaned = {k: v for k, v in cost_mapping.items() if not pd.isna(k[0])}
# # cost_mapping_cleaned

sheet.worksheet('data_2').clear()
set_with_dataframe(sheet.worksheet('data_2'), final_result)

# sheet.worksheet('data_2').clear()

# df_log.drop_duplicates(subset=['id', 'rbb_schedule'], keep='last', inplace=True)
# df_log = df_log[['id', 'rbb_schedule', 'logistic', 'type']]

# set_with_dataframe(sheet.worksheet('data_2'), df_log)

"""#test

"""

from IPython.display import Javascript
display(Javascript('alert("Script execution completed!");'))

# df_3pl_cost = pd.read_csv('https://docs.google.com/spreadsheets/d/16Nk5NHKQYj4DGhivT9obUqjNdPd3VcxFXD0GFRTNrcY/export?format=csv&gid=1816466187')
# df_3pl_cost = df_3pl_cost[df_3pl_cost['cost'].notna()]
# df_3pl_cost['cost'] = df_3pl_cost['cost'].str.replace(',','').astype(int)

# log_area_list = df_log[df_log['area'].notna()]['area'].tolist()

# def define_area(x):
#   for i in log_area_list:
#     if x.upper() == 'LAMPUNG':
#       return 'LAMPUNG'
#     elif (x.lower()+'_' in i.lower()+'_'):
#       return i
#     elif (i.lower()+'_' in x.lower()+'_'):
#       return i

#   return None

# df_3pl_cost['log_area'] = df_3pl_cost['area'].apply(define_area)

# df_log['area_lower'] = df_log['area'].str.strip().str.lower()
# df_log['logistic_lower'] = df_log['logistic'].str.strip().str.lower()
# df_3pl_cost['area_lower'] = df_3pl_cost['log_area'].str.strip().str.lower()
# df_3pl_cost['logistic_lower'] = df_3pl_cost['logistic'].str.strip().str.lower()

# cost_mapping = df_3pl_cost.set_index(['area_lower', 'logistic_lower'])['cost'].to_dict()

# def get_3pl_cost(row):
#     return cost_mapping.get((row['area_lower'], row['logistic_lower']), np.nan)

# df_log['3pl_cost'] = df_log.apply(get_3pl_cost, axis=1)

# df_log.loc[df_log['logistic'].str.contains('indopaket', case=False, na=False), '3pl_cost'] = 44500

# df_log.drop(columns=['area_lower', 'logistic_lower'], inplace=True)